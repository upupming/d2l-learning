{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0d87b76f1af467fd3f493e429fbbcf63b2a04674deb17aebee11ad67a1a0c6c4d",
   "display_name": "Python 3.9.2 64-bit ('py3.9': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VNet 有可用的预训练模型吗？使用预训练模型的增益到底大不大？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutions are all applied with appropriate padding.\n",
    "# 卷积层都不改变 Volume 尺寸，只是改变通道数量\n",
    "# Down Conv 用来改变 Volume 尺寸\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels:int, output_channels:int, num_conv=3, use_1x1conv=False, stride=1, res_channels=None):\n",
    "        \"\"\"之所以有 `res_channels`，是因为残差可以由自定义，不定义的话，默认就是输入 X\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([])\n",
    "        self.bns = nn.ModuleList([])\n",
    "        self.input_channels = input_channels\n",
    "        self.num_conv = num_conv\n",
    "        for i in range(num_conv):\n",
    "            self.convs.append(nn.Conv3d(input_channels, output_channels, kernel_size=5, stride=stride, padding=2))\n",
    "            self.bns.append(nn.BatchNorm3d(output_channels))\n",
    "            input_channels = output_channels\n",
    "        if use_1x1conv:\n",
    "            if res_channels is None:\n",
    "                res_channels = self.input_channels\n",
    "            self.conv1x1 = nn.Conv3d(res_channels, \n",
    "            output_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.conv1x1 = None\n",
    "    def forward(self, X, res=None):\n",
    "        Y = X\n",
    "        # 和李沐讲的不同的是，VNet 直接用的 ReLU Last，也就是最后一个就是在 Y+=X 之前做 ReLU 而不是 Y+=X 之后再做 ReLU\n",
    "        for i in range(self.num_conv):\n",
    "            Y = F.relu(self.bns[i](self.convs[i](Y)))\n",
    "        if self.conv1x1:\n",
    "            if res is None:\n",
    "                X = self.conv1x1(X)\n",
    "            else:\n",
    "                X = self.conv1x1(res)\n",
    "        Y += X\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 128, 128, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "blk = Residual(1, 16, 1, True)\n",
    "# Batch size, channels, Height, Width, Depth\n",
    "X = torch.rand((1, 1, 128, 128, 64))\n",
    "Y = blk(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 32, 64, 64, 32])\n"
     ]
    }
   ],
   "source": [
    "downConv = nn.Conv3d(16, 32, kernel_size=2, stride=2)\n",
    "Y1 = downConv(Y)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 32, 64, 64, 32])\n"
     ]
    }
   ],
   "source": [
    "blk1 = Residual(32, 32, 2, False)\n",
    "Y2 = blk1(Y1)\n",
    "print(Y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Residual(\n  (convs): ModuleList(\n    (0): Conv3d(1, 16, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n  )\n  (bns): ModuleList(\n    (0): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (conv1x1): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n)\nResidual(\n  (convs): ModuleList(\n    (0): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n    (1): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n  )\n  (bns): ModuleList(\n    (0): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(blk)\n",
    "print(blk1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 16, 128, 128, 64])\ntorch.Size([1, 32, 64, 64, 32])\ntorch.Size([1, 64, 32, 32, 16])\ntorch.Size([1, 128, 16, 16, 8])\ntorch.Size([1, 256, 8, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, channels=(16, 32, 64, 128, 256), num_conv=(1, 2, 3, 3, 3)):\n",
    "        super().__init__()\n",
    "        self.num_stages = len(num_conv)\n",
    "        self.enc_blks = nn.ModuleList([])\n",
    "        self.down_convs = nn.ModuleList([])\n",
    "        for i in range(self.num_stages):\n",
    "            # 输入输出通道不同时，才需要使用 1*1 卷积\n",
    "            use_1x1conv = i == 0\n",
    "            # ResNet 模块\n",
    "            if i == 0:\n",
    "                self.enc_blks.append(Residual(1, channels[i], num_conv[i], use_1x1conv=True))\n",
    "            else:\n",
    "                self.enc_blks.append(Residual(channels[i], channels[i], num_conv[i], use_1x1conv=False))\n",
    "            # 最后一个 stage 不需要 Down Conv 模块 \n",
    "            if (i != self.num_stages - 1):\n",
    "                # Down Conv 模块\n",
    "                self.down_convs.append(nn.Conv3d(channels[i], channels[i+1], kernel_size=2, stride=2))\n",
    "    def forward(self, X):\n",
    "        ftrs = []\n",
    "        for i in range(self.num_stages):\n",
    "            X = self.enc_blks[i](X)\n",
    "            ftrs.append(X)\n",
    "            if i != self.num_stages - 1:\n",
    "                X = F.relu(self.down_convs[i](X))\n",
    "        return ftrs\n",
    "\n",
    "encoder = Encoder()\n",
    "ftrs = encoder(X)\n",
    "for ftr in ftrs:\n",
    "    print(ftr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Encoder(\n  (enc_blks): ModuleList(\n    (0): Residual(\n      (convs): ModuleList(\n        (0): Conv3d(1, 16, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n      )\n      (bns): ModuleList(\n        (0): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (conv1x1): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n    )\n    (1): Residual(\n      (convs): ModuleList(\n        (0): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n        (1): Conv3d(32, 32, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n      )\n      (bns): ModuleList(\n        (0): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): Residual(\n      (convs): ModuleList(\n        (0): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n        (1): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n        (2): Conv3d(64, 64, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n      )\n      (bns): ModuleList(\n        (0): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): Residual(\n      (convs): ModuleList(\n        (0): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n        (1): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n        (2): Conv3d(128, 128, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n      )\n      (bns): ModuleList(\n        (0): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): Residual(\n      (convs): ModuleList(\n        (0): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n        (1): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n        (2): Conv3d(256, 256, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=(2, 2, 2))\n      )\n      (bns): ModuleList(\n        (0): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (down_convs): ModuleList(\n    (0): Conv3d(16, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n    (1): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n    (2): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n    (3): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels=(256, 128, 64, 32), num_conv=(1, 2, 3, 3, 3)):\n",
    "        super().__init__()\n",
    "        self.num_stages = len(channels)\n",
    "        self.up_convs = nn.ModuleList([])\n",
    "        self.dec_blks = nn.ModuleList([])\n",
    "        for i in range(self.num_stages):\n",
    "            if i == 0:\n",
    "                in_chs, out_chs = 256, channels[i]//2\n",
    "            else:\n",
    "                in_chs, out_chs = channels[i-1], channels[i]//2\n",
    "            self.up_convs.append(nn.ConvTranspose3d(in_chs, out_chs, kernel_size=2, stride=2))\n",
    "            self.dec_blks.append(Residual(channels[i], channels[i], num_conv[i], use_1x1conv=True, stride=1, res_channels=out_chs))\n",
    "        \n",
    "    def forward(self, X, ftrs):\n",
    "        for i in range(self.num_stages):\n",
    "            X = res = F.relu(self.up_convs[i](X))\n",
    "            X = torch.cat([X, ftrs[i]], dim=1)\n",
    "            X = self.dec_blks[i](X, res)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 32, 128, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder()\n",
    "X = torch.rand(1, 256, 8, 8, 4)\n",
    "Y = decoder(X, ftrs[::-1][1:])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 2, 128, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "head = nn.Conv3d(32, 2, 1)\n",
    "Y1 = head(Y)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         ...,\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "smx = F.softmax(Y1, dim=1)\n",
    "# 在前景/背景上作和为概率 1\n",
    "print(smx.sum(dim=1))"
   ]
  }
 ]
}